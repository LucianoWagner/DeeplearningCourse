{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "073bf8f9",
   "metadata": {},
   "source": [
    "# LangChain: Models, Prompts and Output Parsers\n",
    "\n",
    "\n",
    "## Outline\n",
    "\n",
    " * Direct API calls to OpenAI\n",
    " * API calls through LangChain:\n",
    "   * Prompts\n",
    "   * Models\n",
    "   * Output parsers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01ff606",
   "metadata": {},
   "source": [
    "## Get your [OpenAI API Key](https://platform.openai.com/account/api-keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "70aa2619",
   "metadata": {
    "height": 47,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install python-dotenv\n",
    "#!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b7ed03ed-1322-49e3-b2a2-33e94fb592ef",
   "metadata": {
    "height": 115,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "openai.api_key = os.environ['GROQ_API_KEY']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719a92fb-8227-4513-8950-c965b822c425",
   "metadata": {},
   "source": [
    "Note: LLM's do not always produce the same results. When executing the code in your notebook, you may get slightly different answers that those in the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4336d784-65c2-4a11-8489-b445b1fad177",
   "metadata": {
    "height": 234
   },
   "outputs": [],
   "source": [
    "# 1. Definimos el modelo actualizado\n",
    "# 'llama-3.1-8b-instant' es el reemplazo directo y actual.\n",
    "llm_model = \"llama-3.1-8b-instant\"\n",
    "\n",
    "# 2. Re-configuramos el cliente de Groq (para get_completion)\n",
    "from openai import OpenAI\n",
    "client = OpenAI(\n",
    "    base_url=\"https://api.groq.com/openai/v1\",\n",
    "    api_key=os.environ.get(\"GROQ_API_KEY\")\n",
    ")\n",
    "\n",
    "# 3. Re-configuramos el objeto de LangChain (para chat.invoke)\n",
    "from langchain_groq import ChatGroq\n",
    "chat = ChatGroq(model_name=\"llama-3.1-8b-instant\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbad9cdb",
   "metadata": {},
   "source": [
    "## Chat API : OpenAI\n",
    "\n",
    "Let's start with a direct API calls to OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "484bfa6a",
   "metadata": {
    "height": 166,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_completion(prompt, model=llm_model):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0, \n",
    "    )\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a1d076ce",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 + 1 = 2.\n"
     ]
    }
   ],
   "source": [
    "print(get_completion(\"What is 1+1?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1b32b57a",
   "metadata": {
    "height": 149,
    "tags": []
   },
   "outputs": [],
   "source": [
    "customer_email = \"\"\"\n",
    "Arrr, I be fuming that me blender lid \\\n",
    "flew off and splattered me kitchen walls \\\n",
    "with smoothie! And to make matters worse,\\\n",
    "the warranty don't cover the cost of \\\n",
    "cleaning up me kitchen. I need yer help \\\n",
    "right now, matey!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "18c34459",
   "metadata": {
    "height": 64,
    "tags": []
   },
   "outputs": [],
   "source": [
    "style = \"\"\"American English \\\n",
    "in a calm and respectful tone\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "80b558e2",
   "metadata": {
    "height": 132,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate the text that is delimited by triple backticks \n",
      "into a style that is American English in a calm and respectful tone\n",
      ".\n",
      "text: ```\n",
      "Arrr, I be fuming that me blender lid flew off and splattered me kitchen walls with smoothie! And to make matters worse,the warranty don't cover the cost of cleaning up me kitchen. I need yer help right now, matey!\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"Translate the text \\\n",
    "that is delimited by triple backticks \n",
    "into a style that is {style}.\n",
    "text: ```{customer_email}```\n",
    "\"\"\"\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c883dcbd",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's the translation:\n",
      "\n",
      "\"I'm extremely frustrated that my blender lid flew off and splattered smoothie all over my kitchen walls. To make matters worse, the warranty doesn't cover the cost of cleaning up the mess. I could really use your help right now.\"\n"
     ]
    }
   ],
   "source": [
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "99b33f61",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here\\'s the translation:\\n\\n\"I\\'m extremely frustrated that my blender lid flew off and splattered smoothie all over my kitchen walls. To make matters worse, the warranty doesn\\'t cover the cost of cleaning up the mess. I could really use your help right now.\"'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80482d1",
   "metadata": {},
   "source": [
    "## Chat API : LangChain\n",
    "\n",
    "Let's try how we can do the same using LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "3a525b58",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install --upgrade langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25c5b27",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f0d4a269",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "1cc0c8b8",
   "metadata": {
    "height": 81,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(profile={}, client=<openai.resources.chat.completions.completions.Completions object at 0x00000232D7E4E350>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x00000232D7E4C650>, root_client=<openai.OpenAI object at 0x00000232D7D5B7D0>, root_async_client=<openai.AsyncOpenAI object at 0x00000232D7E285D0>, model_name='llama-3.1-8b-instant', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********'), openai_api_base='https://api.groq.com/openai/v1')"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To control the randomness and creativity of the generated\n",
    "# text by an LLM, use temperature = 0.0\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.0, \n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    openai_api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
    "    base_url=\"https://api.groq.com/openai/v1\"\n",
    ")\n",
    "chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d07256",
   "metadata": {},
   "source": [
    "### Prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "57bda7d8",
   "metadata": {
    "height": 98,
    "tags": []
   },
   "outputs": [],
   "source": [
    "template_string = \"\"\"Translate the text \\\n",
    "that is delimited by triple backticks \\\n",
    "into a style that is {style}. \\\n",
    "text: ```{text}```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "3a31f246",
   "metadata": {
    "height": 81,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt_template = ChatPromptTemplate.from_template(template_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "cac2cb16",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['style', 'text'], input_types={}, partial_variables={}, template='Translate the text that is delimited by triple backticks into a style that is {style}. text: ```{text}```\\n')"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.messages[0].prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "cdc5566c",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['style', 'text']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.messages[0].prompt.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "bbd51a93",
   "metadata": {
    "height": 64,
    "tags": []
   },
   "outputs": [],
   "source": [
    "customer_style = \"\"\"American English \\\n",
    "in a calm and respectful tone\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "48989d11",
   "metadata": {
    "height": 149,
    "tags": []
   },
   "outputs": [],
   "source": [
    "customer_email = \"\"\"\n",
    "Arrr, I be fuming that me blender lid \\\n",
    "flew off and splattered me kitchen walls \\\n",
    "with smoothie! And to make matters worse, \\\n",
    "the warranty don't cover the cost of \\\n",
    "cleaning up me kitchen. I need yer help \\\n",
    "right now, matey!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "dff3954f",
   "metadata": {
    "height": 64,
    "tags": []
   },
   "outputs": [],
   "source": [
    "customer_messages = prompt_template.format_messages(\n",
    "                    style=customer_style,\n",
    "                    text=customer_email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "8c09d8b4",
   "metadata": {
    "height": 47,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'langchain_core.messages.human.HumanMessage'>\n"
     ]
    }
   ],
   "source": [
    "print(type(customer_messages))\n",
    "print(type(customer_messages[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "e02dafa2",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Translate the text that is delimited by triple backticks into a style that is American English in a calm and respectful tone\\n. text: ```\\nArrr, I be fuming that me blender lid flew off and splattered me kitchen walls with smoothie! And to make matters worse, the warranty don't cover the cost of cleaning up me kitchen. I need yer help right now, matey!\\n```\\n\" additional_kwargs={} response_metadata={}\n"
     ]
    }
   ],
   "source": [
    "print(customer_messages[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "bd789f9f",
   "metadata": {
    "height": 47,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Here\\'s the translation of the text into American English in a calm and respectful tone:\\n\\n\"I\\'m really frustrated that my blender lid flew off and spilled my smoothie all over my kitchen walls. To make matters worse, the warranty doesn\\'t cover the cost of cleaning up the mess. I could really use your help right now.\"' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 66, 'prompt_tokens': 117, 'total_tokens': 183, 'completion_tokens_details': None, 'prompt_tokens_details': None, 'queue_time': 0.150248832, 'prompt_time': 0.007120915, 'completion_time': 0.101076542, 'total_time': 0.108197457}, 'model_provider': 'openai', 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_9ca2574dca', 'id': 'chatcmpl-3284cee2-9970-4633-b351-fd9884e92aa3', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--019b9484-720c-7e60-bc2a-a9f20ee70af9-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 117, 'output_tokens': 66, 'total_tokens': 183, 'input_token_details': {}, 'output_token_details': {}}\n"
     ]
    }
   ],
   "source": [
    "# Call the LLM to translate to the style of the customer message\n",
    "customer_response = chat.invoke(customer_messages)\n",
    "print(customer_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "ad294407",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's the translation of the text into American English in a calm and respectful tone:\n",
      "\n",
      "\"I'm really frustrated that my blender lid flew off and spilled my smoothie all over my kitchen walls. To make matters worse, the warranty doesn't cover the cost of cleaning up the mess. I could really use your help right now.\"\n"
     ]
    }
   ],
   "source": [
    "print(customer_response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "7c267e5f",
   "metadata": {
    "height": 166,
    "tags": []
   },
   "outputs": [],
   "source": [
    "service_reply = \"\"\"Hey there customer, \\\n",
    "the warranty does not cover \\\n",
    "cleaning expenses for your kitchen \\\n",
    "because it's your fault that \\\n",
    "you misused your blender \\\n",
    "by forgetting to put the lid on before \\\n",
    "starting the blender. \\\n",
    "Tough luck! See ya!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "2ff72bd1",
   "metadata": {
    "height": 81,
    "tags": []
   },
   "outputs": [],
   "source": [
    "service_style_pirate = \"\"\"\\\n",
    "a polite tone \\\n",
    "that speaks in English Pirate\\\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "7d9e8f3f",
   "metadata": {
    "height": 98,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate the text that is delimited by triple backticks into a style that is a polite tone that speaks in English Pirate. text: ```Hey there customer, the warranty does not cover cleaning expenses for your kitchen because it's your fault that you misused your blender by forgetting to put the lid on before starting the blender. Tough luck! See ya!\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "service_messages = prompt_template.format_messages(\n",
    "    style=service_style_pirate,\n",
    "    text=service_reply)\n",
    "\n",
    "print(service_messages[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a0ae5552",
   "metadata": {
    "height": 47,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```Hey there customer, I be afraid the warranty don't cover the cost o' cleanin' yer kitchen, matey. It seems ye had a wee mishap with yer blender, forgettin' to put the lid on before set-tin' it sail. A bit o' carelessness, if ye don't mind me sayin' so. I be hopin' ye'll take heed o' this lesson and be more mindful o' yer kitchen contraptions in the future. Fair winds and following seas to ye, but I be afraid ye'll be footin' the bill for this one, matey!```\n"
     ]
    }
   ],
   "source": [
    "service_response = chat.invoke(service_messages)\n",
    "print(service_response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36536e79",
   "metadata": {},
   "source": [
    "## Output Parsers\n",
    "\n",
    "Let's start with defining how we would like the LLM output to look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "f1ccdff5",
   "metadata": {
    "height": 98,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gift': False, 'delivery_days': 5, 'price_value': 'pretty affordable!'}"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "  \"gift\": False,\n",
    "  \"delivery_days\": 5,\n",
    "  \"price_value\": \"pretty affordable!\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "df0f4680",
   "metadata": {
    "height": 540,
    "tags": []
   },
   "outputs": [],
   "source": [
    "customer_review = \"\"\"\\\n",
    "This leaf blower is pretty amazing.  It has four settings:\\\n",
    "candle blower, gentle breeze, windy city, and tornado. \\\n",
    "It arrived in two days, just in time for my wife's \\\n",
    "anniversary present. \\\n",
    "I think my wife liked it so much she was speechless. \\\n",
    "So far I've been the only one using it, and I've been \\\n",
    "using it every other morning to clear the leaves on our lawn. \\\n",
    "It's slightly more expensive than the other leaf blowers \\\n",
    "out there, but I think it's worth it for the extra features.\n",
    "\"\"\"\n",
    "\n",
    "review_template = \"\"\"\\\n",
    "For the following text, extract the following information:\n",
    "\n",
    "gift: Was the item purchased as a gift for someone else? \\\n",
    "Answer True if yes, False if not or unknown.\n",
    "\n",
    "delivery_days: How many days did it take for the product \\\n",
    "to arrive? If this information is not found, output -1.\n",
    "\n",
    "price_value: Extract any sentences about the value or price,\\\n",
    "and output them as a comma separated Python list.\n",
    "\n",
    "Format the output as JSON with the following keys:\n",
    "gift\n",
    "delivery_days\n",
    "price_value\n",
    "\n",
    "text: {text}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "f2386e9c",
   "metadata": {
    "height": 81,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Importación exitosa!\n"
     ]
    }
   ],
   "source": [
    "# Importaciones para LangChain v0.2 / v0.3\n",
    "from langchain_classic.output_parsers import ResponseSchema, StructuredOutputParser\n",
    "# Verificamos que funcione definiendo un esquema rápido\n",
    "gift_schema = ResponseSchema(name=\"gift\", description=\"Es un regalo?\")\n",
    "output_parser = StructuredOutputParser.from_response_schemas([gift_schema])\n",
    "\n",
    "print(\"¡Importación exitosa!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "121bb0d4",
   "metadata": {
    "height": 81,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_classic.output_parsers import ResponseSchema\n",
    "from langchain_classic.output_parsers import StructuredOutputParser\n",
    "from langchain_classic.chat_models import ChatOpenAI\n",
    "from langchain_classic.prompts import ChatPromptTemplate\n",
    "\n",
    "# 1. Definimos qué queremos extraer\n",
    "monto_schema = ResponseSchema(\n",
    "    name=\"monto\",\n",
    "    description=\"El valor monetario de la transferencia. Solo el número.\"\n",
    ")\n",
    "destinatario_schema = ResponseSchema(\n",
    "    name=\"destinatario\",\n",
    "    description=\"El nombre de la persona o empresa que recibe el dinero.\"\n",
    ")\n",
    "banco_schema = ResponseSchema(\n",
    "    name=\"banco\",\n",
    "    description=\"El nombre del banco destino. Si no se menciona, poner 'Desconocido'.\"\n",
    ")\n",
    "\n",
    "# Metemos todo en una lista\n",
    "response_schemas = [monto_schema, destinatario_schema, banco_schema]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "10de1d28",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 2. Creamos el parser\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "\n",
    "# 3. Generamos las instrucciones automáticas (el texto técnico para el prompt)\n",
    "format_instructions = output_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "3a3c0609",
   "metadata": {
    "height": 81,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Resultado como Diccionario ---\n",
      "{'monto': '5500', 'destinatario': 'Juan Perez', 'banco': 'Banco Galicia'}\n"
     ]
    }
   ],
   "source": [
    "# 4. Definimos el mensaje del usuario\n",
    "mensaje_usuario = \"Le mandé 5500 pesos a Juan Perez que tiene cuenta en el Banco Galicia.\"\n",
    "\n",
    "# 5. Armamos la plantilla incluyendo las instrucciones de formato\n",
    "template = \"\"\"\n",
    "Analiza el siguiente mensaje de una transferencia bancaria y extrae la información solicitada.\n",
    "\n",
    "Mensaje: {texto}\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template=template)\n",
    "messages = prompt.format_messages(\n",
    "    texto=mensaje_usuario, \n",
    "    format_instructions=format_instructions\n",
    ")\n",
    "\n",
    "# 6. Llamamos a Groq (usando el objeto 'chat' que configuramos antes)\n",
    "response = chat.invoke(messages)\n",
    "# 7. ¡EL PASO CLAVE! Parseamos el string a un diccionario de Python\n",
    "output_dict = output_parser.parse(response.content)\n",
    "# 8. Verificación de resultados\n",
    "print(\"--- Resultado como Diccionario ---\")\n",
    "print(output_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc3e883",
   "metadata": {},
   "source": [
    "#MEMORY:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b57825c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "\"ChatOpenAI\" object has no field \"get_num_tokens_from_messages\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[125], line 11\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmock_get_num_tokens_from_messages\u001b[39m(messages):\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# Usamos una aproximación o el contador de gpt-3.5\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m chat\u001b[38;5;241m.\u001b[39mget_num_tokens(\u001b[38;5;28mstr\u001b[39m(messages))\n\u001b[1;32m---> 11\u001b[0m \u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_num_tokens_from_messages\u001b[49m \u001b[38;5;241m=\u001b[39m mock_get_num_tokens_from_messages \n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# 1. Configuramos el LLM principal\u001b[39;00m\n\u001b[0;32m     14\u001b[0m llm \u001b[38;5;241m=\u001b[39m ChatOpenAI(\n\u001b[0;32m     15\u001b[0m     temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m     16\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama-3.1-8b-instant\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     17\u001b[0m     api_key\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGROQ_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     18\u001b[0m     base_url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://api.groq.com/openai/v1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     19\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\lucia\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pydantic\\main.py:1032\u001b[0m, in \u001b[0;36mBaseModel.__setattr__\u001b[1;34m(self, name, value)\u001b[0m\n\u001b[0;32m   1030\u001b[0m     setattr_handler(\u001b[38;5;28mself\u001b[39m, name, value)\n\u001b[0;32m   1031\u001b[0m \u001b[38;5;66;03m# if None is returned from _setattr_handler, the attribute was set directly\u001b[39;00m\n\u001b[1;32m-> 1032\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (setattr_handler \u001b[38;5;241m:=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setattr_handler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1033\u001b[0m     setattr_handler(\u001b[38;5;28mself\u001b[39m, name, value)  \u001b[38;5;66;03m# call here to not memo on possibly unknown fields\u001b[39;00m\n\u001b[0;32m   1034\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__pydantic_setattr_handlers__[name] \u001b[38;5;241m=\u001b[39m setattr_handler\n",
      "File \u001b[1;32mc:\\Users\\lucia\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pydantic\\main.py:1079\u001b[0m, in \u001b[0;36mBaseModel._setattr_handler\u001b[1;34m(self, name, value)\u001b[0m\n\u001b[0;32m   1076\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__pydantic_fields__:\n\u001b[0;32m   1077\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mextra\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mallow\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m   1078\u001b[0m         \u001b[38;5;66;03m# TODO - matching error\u001b[39;00m\n\u001b[1;32m-> 1079\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m object has no field \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m   1080\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m attr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1081\u001b[0m         \u001b[38;5;66;03m# attribute does not exist, so put it in extra\u001b[39;00m\n\u001b[0;32m   1082\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__pydantic_extra__[name] \u001b[38;5;241m=\u001b[39m value\n",
      "\u001b[1;31mValueError\u001b[0m: \"ChatOpenAI\" object has no field \"get_num_tokens_from_messages\""
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "import os\n",
    "\n",
    "\n",
    "# 1. Configuramos el LLM principal\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0,\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
    "    base_url=\"https://api.groq.com/openai/v1\"\n",
    ")\n",
    "\n",
    "# 2. Configuramos la Memoria de Resumen\n",
    "# max_token_limit: Es el umbral. Si la charla supera esto, empieza a resumir.\n",
    "# Ponemos un número bajo (ej: 100) para ver el resumen rápido.\n",
    "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)\n",
    "\n",
    "# 3. Creamos la cadena\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory=memory,\n",
    "    verbose=True  # VITAL para ver el resumen en consola\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "a297325a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hola, soy Luciano y vivo en Argentina. Trabajo como Analista Programador.\n",
      "AI:\u001b[0m\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "get_num_tokens_from_messages() is not presently implemented for model llama-3.1-8b-instant. See https://platform.openai.com/docs/guides/text-generation/managing-tokens for information on how messages are converted to tokens.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[124], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Paso 1: Introducción\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mconversation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHola, soy Luciano y vivo en Argentina. Trabajo como Analista Programador.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lucia\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\chains\\llm.py:318\u001b[0m, in \u001b[0;36mLLMChain.predict\u001b[1;34m(self, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, callbacks: Callbacks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    304\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Format prompt with kwargs and pass to LLM.\u001b[39;00m\n\u001b[0;32m    305\u001b[0m \n\u001b[0;32m    306\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;124;03m            completion = llm.predict(adjective=\"funny\")\u001b[39;00m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 318\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key]\n",
      "File \u001b[1;32mc:\\Users\\lucia\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:206\u001b[0m, in \u001b[0;36mwarning_emitting_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;66;03m# Can't set new_doc on some extension objects.\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39msuppress(\u001b[38;5;167;01mAttributeError\u001b[39;00m):\n\u001b[1;32m--> 206\u001b[0m     obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__doc__\u001b[39m \u001b[38;5;241m=\u001b[39m new_doc\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwarn_if_direct_instance\u001b[39m(\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28mself\u001b[39m: Any, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any\n\u001b[0;32m    210\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    211\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Warn that the class is in beta.\"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lucia\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\chains\\base.py:389\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    357\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[0;32m    358\u001b[0m \n\u001b[0;32m    359\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;124;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    382\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks,\n\u001b[0;32m    384\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m: tags,\n\u001b[0;32m    385\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[0;32m    386\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_name,\n\u001b[0;32m    387\u001b[0m }\n\u001b[1;32m--> 389\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    393\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    394\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lucia\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\chains\\base.py:170\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    169\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 170\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    171\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "File \u001b[1;32mc:\\Users\\lucia\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\chains\\base.py:165\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[0;32m    159\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs, run_manager\u001b[38;5;241m=\u001b[39mrun_manager)\n\u001b[0;32m    161\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    162\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[0;32m    163\u001b[0m     )\n\u001b[1;32m--> 165\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprep_outputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    169\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32mc:\\Users\\lucia\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\chains\\base.py:466\u001b[0m, in \u001b[0;36mChain.prep_outputs\u001b[1;34m(self, inputs, outputs, return_only_outputs)\u001b[0m\n\u001b[0;32m    464\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_outputs(outputs)\n\u001b[0;32m    465\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 466\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmemory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_only_outputs:\n\u001b[0;32m    468\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32mc:\\Users\\lucia\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\memory\\summary_buffer.py:96\u001b[0m, in \u001b[0;36mConversationSummaryBufferMemory.save_context\u001b[1;34m(self, inputs, outputs)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Save context from this conversation to buffer.\"\"\"\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39msave_context(inputs, outputs)\n\u001b[1;32m---> 96\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprune\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lucia\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\memory\\summary_buffer.py:108\u001b[0m, in \u001b[0;36mConversationSummaryBufferMemory.prune\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Prune buffer if it exceeds max token limit\"\"\"\u001b[39;00m\n\u001b[0;32m    107\u001b[0m buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchat_memory\u001b[38;5;241m.\u001b[39mmessages\n\u001b[1;32m--> 108\u001b[0m curr_buffer_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_num_tokens_from_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m curr_buffer_length \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_token_limit:\n\u001b[0;32m    110\u001b[0m     pruned_memory \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\lucia\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1761\u001b[0m, in \u001b[0;36mget_num_tokens_from_messages\u001b[1;34m(self, messages, tools)\u001b[0m\n\u001b[0;32m   1535\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mChatOpenAI\u001b[39;00m(BaseChatOpenAI):  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"OpenAI chat model integration.\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \n\u001b[0;32m   1538\u001b[0m \u001b[38;5;124;03m    .. dropdown:: Setup\u001b[39;00m\n\u001b[0;32m   1539\u001b[0m \u001b[38;5;124;03m        :open:\u001b[39;00m\n\u001b[0;32m   1540\u001b[0m \n\u001b[0;32m   1541\u001b[0m \u001b[38;5;124;03m        Install ``langchain-openai`` and set environment variable ``OPENAI_API_KEY``.\u001b[39;00m\n\u001b[0;32m   1542\u001b[0m \n\u001b[0;32m   1543\u001b[0m \u001b[38;5;124;03m        .. code-block:: bash\u001b[39;00m\n\u001b[0;32m   1544\u001b[0m \n\u001b[0;32m   1545\u001b[0m \u001b[38;5;124;03m            pip install -U langchain-openai\u001b[39;00m\n\u001b[0;32m   1546\u001b[0m \u001b[38;5;124;03m            export OPENAI_API_KEY=\"your-api-key\"\u001b[39;00m\n\u001b[0;32m   1547\u001b[0m \n\u001b[0;32m   1548\u001b[0m \u001b[38;5;124;03m    .. dropdown:: Key init args — completion params\u001b[39;00m\n\u001b[0;32m   1549\u001b[0m \n\u001b[0;32m   1550\u001b[0m \u001b[38;5;124;03m        model: str\u001b[39;00m\n\u001b[0;32m   1551\u001b[0m \u001b[38;5;124;03m            Name of OpenAI model to use.\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;124;03m        temperature: float\u001b[39;00m\n\u001b[0;32m   1553\u001b[0m \u001b[38;5;124;03m            Sampling temperature.\u001b[39;00m\n\u001b[0;32m   1554\u001b[0m \u001b[38;5;124;03m        max_tokens: Optional[int]\u001b[39;00m\n\u001b[0;32m   1555\u001b[0m \u001b[38;5;124;03m            Max number of tokens to generate.\u001b[39;00m\n\u001b[0;32m   1556\u001b[0m \u001b[38;5;124;03m        logprobs: Optional[bool]\u001b[39;00m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;124;03m            Whether to return logprobs.\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;124;03m        stream_options: Dict\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;124;03m            Configure streaming outputs, like whether to return token usage when\u001b[39;00m\n\u001b[0;32m   1560\u001b[0m \u001b[38;5;124;03m            streaming (``{\"include_usage\": True}``).\u001b[39;00m\n\u001b[0;32m   1561\u001b[0m \n\u001b[0;32m   1562\u001b[0m \u001b[38;5;124;03m        See full list of supported init args and their descriptions in the params section.\u001b[39;00m\n\u001b[0;32m   1563\u001b[0m \n\u001b[0;32m   1564\u001b[0m \u001b[38;5;124;03m    .. dropdown:: Key init args — client params\u001b[39;00m\n\u001b[0;32m   1565\u001b[0m \n\u001b[0;32m   1566\u001b[0m \u001b[38;5;124;03m        timeout: Union[float, Tuple[float, float], Any, None]\u001b[39;00m\n\u001b[0;32m   1567\u001b[0m \u001b[38;5;124;03m            Timeout for requests.\u001b[39;00m\n\u001b[0;32m   1568\u001b[0m \u001b[38;5;124;03m        max_retries: int\u001b[39;00m\n\u001b[0;32m   1569\u001b[0m \u001b[38;5;124;03m            Max number of retries.\u001b[39;00m\n\u001b[0;32m   1570\u001b[0m \u001b[38;5;124;03m        api_key: Optional[str]\u001b[39;00m\n\u001b[0;32m   1571\u001b[0m \u001b[38;5;124;03m            OpenAI API key. If not passed in will be read from env var OPENAI_API_KEY.\u001b[39;00m\n\u001b[0;32m   1572\u001b[0m \u001b[38;5;124;03m        base_url: Optional[str]\u001b[39;00m\n\u001b[0;32m   1573\u001b[0m \u001b[38;5;124;03m            Base URL for API requests. Only specify if using a proxy or service\u001b[39;00m\n\u001b[0;32m   1574\u001b[0m \u001b[38;5;124;03m            emulator.\u001b[39;00m\n\u001b[0;32m   1575\u001b[0m \u001b[38;5;124;03m        organization: Optional[str]\u001b[39;00m\n\u001b[0;32m   1576\u001b[0m \u001b[38;5;124;03m            OpenAI organization ID. If not passed in will be read from env\u001b[39;00m\n\u001b[0;32m   1577\u001b[0m \u001b[38;5;124;03m            var OPENAI_ORG_ID.\u001b[39;00m\n\u001b[0;32m   1578\u001b[0m \n\u001b[0;32m   1579\u001b[0m \u001b[38;5;124;03m        See full list of supported init args and their descriptions in the params section.\u001b[39;00m\n\u001b[0;32m   1580\u001b[0m \n\u001b[0;32m   1581\u001b[0m \u001b[38;5;124;03m    .. dropdown:: Instantiate\u001b[39;00m\n\u001b[0;32m   1582\u001b[0m \n\u001b[0;32m   1583\u001b[0m \u001b[38;5;124;03m        .. code-block:: python\u001b[39;00m\n\u001b[0;32m   1584\u001b[0m \n\u001b[0;32m   1585\u001b[0m \u001b[38;5;124;03m            from langchain_openai import ChatOpenAI\u001b[39;00m\n\u001b[0;32m   1586\u001b[0m \n\u001b[0;32m   1587\u001b[0m \u001b[38;5;124;03m            llm = ChatOpenAI(\u001b[39;00m\n\u001b[0;32m   1588\u001b[0m \u001b[38;5;124;03m                model=\"gpt-4o\",\u001b[39;00m\n\u001b[0;32m   1589\u001b[0m \u001b[38;5;124;03m                temperature=0,\u001b[39;00m\n\u001b[0;32m   1590\u001b[0m \u001b[38;5;124;03m                max_tokens=None,\u001b[39;00m\n\u001b[0;32m   1591\u001b[0m \u001b[38;5;124;03m                timeout=None,\u001b[39;00m\n\u001b[0;32m   1592\u001b[0m \u001b[38;5;124;03m                max_retries=2,\u001b[39;00m\n\u001b[0;32m   1593\u001b[0m \u001b[38;5;124;03m                # api_key=\"...\",\u001b[39;00m\n\u001b[0;32m   1594\u001b[0m \u001b[38;5;124;03m                # base_url=\"...\",\u001b[39;00m\n\u001b[0;32m   1595\u001b[0m \u001b[38;5;124;03m                # organization=\"...\",\u001b[39;00m\n\u001b[0;32m   1596\u001b[0m \u001b[38;5;124;03m                # other params...\u001b[39;00m\n\u001b[0;32m   1597\u001b[0m \u001b[38;5;124;03m            )\u001b[39;00m\n\u001b[0;32m   1598\u001b[0m \n\u001b[0;32m   1599\u001b[0m \u001b[38;5;124;03m        **NOTE**: Any param which is not explicitly supported will be passed directly to the\u001b[39;00m\n\u001b[0;32m   1600\u001b[0m \u001b[38;5;124;03m        ``openai.OpenAI.chat.completions.create(...)`` API every time to the model is\u001b[39;00m\n\u001b[0;32m   1601\u001b[0m \u001b[38;5;124;03m        invoked. For example:\u001b[39;00m\n\u001b[0;32m   1602\u001b[0m \n\u001b[0;32m   1603\u001b[0m \u001b[38;5;124;03m        .. code-block:: python\u001b[39;00m\n\u001b[0;32m   1604\u001b[0m \n\u001b[0;32m   1605\u001b[0m \u001b[38;5;124;03m            from langchain_openai import ChatOpenAI\u001b[39;00m\n\u001b[0;32m   1606\u001b[0m \u001b[38;5;124;03m            import openai\u001b[39;00m\n\u001b[0;32m   1607\u001b[0m \n\u001b[0;32m   1608\u001b[0m \u001b[38;5;124;03m            ChatOpenAI(..., frequency_penalty=0.2).invoke(...)\u001b[39;00m\n\u001b[0;32m   1609\u001b[0m \n\u001b[0;32m   1610\u001b[0m \u001b[38;5;124;03m            # results in underlying API call of:\u001b[39;00m\n\u001b[0;32m   1611\u001b[0m \n\u001b[0;32m   1612\u001b[0m \u001b[38;5;124;03m            openai.OpenAI(..).chat.completions.create(..., frequency_penalty=0.2)\u001b[39;00m\n\u001b[0;32m   1613\u001b[0m \n\u001b[0;32m   1614\u001b[0m \u001b[38;5;124;03m            # which is also equivalent to:\u001b[39;00m\n\u001b[0;32m   1615\u001b[0m \n\u001b[0;32m   1616\u001b[0m \u001b[38;5;124;03m            ChatOpenAI(...).invoke(..., frequency_penalty=0.2)\u001b[39;00m\n\u001b[0;32m   1617\u001b[0m \n\u001b[0;32m   1618\u001b[0m \u001b[38;5;124;03m    .. dropdown:: Invoke\u001b[39;00m\n\u001b[0;32m   1619\u001b[0m \n\u001b[0;32m   1620\u001b[0m \u001b[38;5;124;03m        .. code-block:: python\u001b[39;00m\n\u001b[0;32m   1621\u001b[0m \n\u001b[0;32m   1622\u001b[0m \u001b[38;5;124;03m            messages = [\u001b[39;00m\n\u001b[0;32m   1623\u001b[0m \u001b[38;5;124;03m                (\u001b[39;00m\n\u001b[0;32m   1624\u001b[0m \u001b[38;5;124;03m                    \"system\",\u001b[39;00m\n\u001b[0;32m   1625\u001b[0m \u001b[38;5;124;03m                    \"You are a helpful translator. Translate the user sentence to French.\",\u001b[39;00m\n\u001b[0;32m   1626\u001b[0m \u001b[38;5;124;03m                ),\u001b[39;00m\n\u001b[0;32m   1627\u001b[0m \u001b[38;5;124;03m                (\"human\", \"I love programming.\"),\u001b[39;00m\n\u001b[0;32m   1628\u001b[0m \u001b[38;5;124;03m            ]\u001b[39;00m\n\u001b[0;32m   1629\u001b[0m \u001b[38;5;124;03m            llm.invoke(messages)\u001b[39;00m\n\u001b[0;32m   1630\u001b[0m \n\u001b[0;32m   1631\u001b[0m \u001b[38;5;124;03m        .. code-block:: pycon\u001b[39;00m\n\u001b[0;32m   1632\u001b[0m \n\u001b[0;32m   1633\u001b[0m \u001b[38;5;124;03m            AIMessage(\u001b[39;00m\n\u001b[0;32m   1634\u001b[0m \u001b[38;5;124;03m                content=\"J'adore la programmation.\",\u001b[39;00m\n\u001b[0;32m   1635\u001b[0m \u001b[38;5;124;03m                response_metadata={\u001b[39;00m\n\u001b[0;32m   1636\u001b[0m \u001b[38;5;124;03m                    \"token_usage\": {\u001b[39;00m\n\u001b[0;32m   1637\u001b[0m \u001b[38;5;124;03m                        \"completion_tokens\": 5,\u001b[39;00m\n\u001b[0;32m   1638\u001b[0m \u001b[38;5;124;03m                        \"prompt_tokens\": 31,\u001b[39;00m\n\u001b[0;32m   1639\u001b[0m \u001b[38;5;124;03m                        \"total_tokens\": 36,\u001b[39;00m\n\u001b[0;32m   1640\u001b[0m \u001b[38;5;124;03m                    },\u001b[39;00m\n\u001b[0;32m   1641\u001b[0m \u001b[38;5;124;03m                    \"model_name\": \"gpt-4o\",\u001b[39;00m\n\u001b[0;32m   1642\u001b[0m \u001b[38;5;124;03m                    \"system_fingerprint\": \"fp_43dfabdef1\",\u001b[39;00m\n\u001b[0;32m   1643\u001b[0m \u001b[38;5;124;03m                    \"finish_reason\": \"stop\",\u001b[39;00m\n\u001b[0;32m   1644\u001b[0m \u001b[38;5;124;03m                    \"logprobs\": None,\u001b[39;00m\n\u001b[0;32m   1645\u001b[0m \u001b[38;5;124;03m                },\u001b[39;00m\n\u001b[0;32m   1646\u001b[0m \u001b[38;5;124;03m                id=\"run-012cffe2-5d3d-424d-83b5-51c6d4a593d1-0\",\u001b[39;00m\n\u001b[0;32m   1647\u001b[0m \u001b[38;5;124;03m                usage_metadata={\"input_tokens\": 31, \"output_tokens\": 5, \"total_tokens\": 36},\u001b[39;00m\n\u001b[0;32m   1648\u001b[0m \u001b[38;5;124;03m            )\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m \n\u001b[0;32m   1650\u001b[0m \u001b[38;5;124;03m    .. dropdown:: Stream\u001b[39;00m\n\u001b[0;32m   1651\u001b[0m \n\u001b[0;32m   1652\u001b[0m \u001b[38;5;124;03m        .. code-block:: python\u001b[39;00m\n\u001b[0;32m   1653\u001b[0m \n\u001b[0;32m   1654\u001b[0m \u001b[38;5;124;03m            for chunk in llm.stream(messages):\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m \u001b[38;5;124;03m                print(chunk)\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m \n\u001b[0;32m   1657\u001b[0m \u001b[38;5;124;03m        .. code-block:: python\u001b[39;00m\n\u001b[0;32m   1658\u001b[0m \n\u001b[0;32m   1659\u001b[0m \u001b[38;5;124;03m            AIMessageChunk(content=\"\", id=\"run-9e1517e3-12bf-48f2-bb1b-2e824f7cd7b0\")\u001b[39;00m\n\u001b[0;32m   1660\u001b[0m \u001b[38;5;124;03m            AIMessageChunk(content=\"J\", id=\"run-9e1517e3-12bf-48f2-bb1b-2e824f7cd7b0\")\u001b[39;00m\n\u001b[0;32m   1661\u001b[0m \u001b[38;5;124;03m            AIMessageChunk(content=\"'adore\", id=\"run-9e1517e3-12bf-48f2-bb1b-2e824f7cd7b0\")\u001b[39;00m\n\u001b[0;32m   1662\u001b[0m \u001b[38;5;124;03m            AIMessageChunk(content=\" la\", id=\"run-9e1517e3-12bf-48f2-bb1b-2e824f7cd7b0\")\u001b[39;00m\n\u001b[0;32m   1663\u001b[0m \u001b[38;5;124;03m            AIMessageChunk(\u001b[39;00m\n\u001b[0;32m   1664\u001b[0m \u001b[38;5;124;03m                content=\" programmation\", id=\"run-9e1517e3-12bf-48f2-bb1b-2e824f7cd7b0\"\u001b[39;00m\n\u001b[0;32m   1665\u001b[0m \u001b[38;5;124;03m            )\u001b[39;00m\n\u001b[0;32m   1666\u001b[0m \u001b[38;5;124;03m            AIMessageChunk(content=\".\", id=\"run-9e1517e3-12bf-48f2-bb1b-2e824f7cd7b0\")\u001b[39;00m\n\u001b[0;32m   1667\u001b[0m \u001b[38;5;124;03m            AIMessageChunk(\u001b[39;00m\n\u001b[0;32m   1668\u001b[0m \u001b[38;5;124;03m                content=\"\",\u001b[39;00m\n\u001b[0;32m   1669\u001b[0m \u001b[38;5;124;03m                response_metadata={\"finish_reason\": \"stop\"},\u001b[39;00m\n\u001b[0;32m   1670\u001b[0m \u001b[38;5;124;03m                id=\"run-9e1517e3-12bf-48f2-bb1b-2e824f7cd7b0\",\u001b[39;00m\n\u001b[0;32m   1671\u001b[0m \u001b[38;5;124;03m            )\u001b[39;00m\n\u001b[0;32m   1672\u001b[0m \n\u001b[0;32m   1673\u001b[0m \u001b[38;5;124;03m        .. code-block:: python\u001b[39;00m\n\u001b[0;32m   1674\u001b[0m \n\u001b[0;32m   1675\u001b[0m \u001b[38;5;124;03m            stream = llm.stream(messages)\u001b[39;00m\n\u001b[0;32m   1676\u001b[0m \u001b[38;5;124;03m            full = next(stream)\u001b[39;00m\n\u001b[0;32m   1677\u001b[0m \u001b[38;5;124;03m            for chunk in stream:\u001b[39;00m\n\u001b[0;32m   1678\u001b[0m \u001b[38;5;124;03m                full += chunk\u001b[39;00m\n\u001b[0;32m   1679\u001b[0m \u001b[38;5;124;03m            full\u001b[39;00m\n\u001b[0;32m   1680\u001b[0m \n\u001b[0;32m   1681\u001b[0m \u001b[38;5;124;03m        .. code-block:: python\u001b[39;00m\n\u001b[0;32m   1682\u001b[0m \n\u001b[0;32m   1683\u001b[0m \u001b[38;5;124;03m            AIMessageChunk(\u001b[39;00m\n\u001b[0;32m   1684\u001b[0m \u001b[38;5;124;03m                content=\"J'adore la programmation.\",\u001b[39;00m\n\u001b[0;32m   1685\u001b[0m \u001b[38;5;124;03m                response_metadata={\"finish_reason\": \"stop\"},\u001b[39;00m\n\u001b[0;32m   1686\u001b[0m \u001b[38;5;124;03m                id=\"run-bf917526-7f58-4683-84f7-36a6b671d140\",\u001b[39;00m\n\u001b[0;32m   1687\u001b[0m \u001b[38;5;124;03m            )\u001b[39;00m\n\u001b[0;32m   1688\u001b[0m \n\u001b[0;32m   1689\u001b[0m \u001b[38;5;124;03m    .. dropdown:: Async\u001b[39;00m\n\u001b[0;32m   1690\u001b[0m \n\u001b[0;32m   1691\u001b[0m \u001b[38;5;124;03m        .. code-block:: python\u001b[39;00m\n\u001b[0;32m   1692\u001b[0m \n\u001b[0;32m   1693\u001b[0m \u001b[38;5;124;03m            await llm.ainvoke(messages)\u001b[39;00m\n\u001b[0;32m   1694\u001b[0m \n\u001b[0;32m   1695\u001b[0m \u001b[38;5;124;03m            # stream:\u001b[39;00m\n\u001b[0;32m   1696\u001b[0m \u001b[38;5;124;03m            # async for chunk in (await llm.astream(messages))\u001b[39;00m\n\u001b[0;32m   1697\u001b[0m \n\u001b[0;32m   1698\u001b[0m \u001b[38;5;124;03m            # batch:\u001b[39;00m\n\u001b[0;32m   1699\u001b[0m \u001b[38;5;124;03m            # await llm.abatch([messages])\u001b[39;00m\n\u001b[0;32m   1700\u001b[0m \n\u001b[0;32m   1701\u001b[0m \u001b[38;5;124;03m        .. code-block:: python\u001b[39;00m\n\u001b[0;32m   1702\u001b[0m \n\u001b[0;32m   1703\u001b[0m \u001b[38;5;124;03m            AIMessage(\u001b[39;00m\n\u001b[0;32m   1704\u001b[0m \u001b[38;5;124;03m                content=\"J'adore la programmation.\",\u001b[39;00m\n\u001b[0;32m   1705\u001b[0m \u001b[38;5;124;03m                response_metadata={\u001b[39;00m\n\u001b[0;32m   1706\u001b[0m \u001b[38;5;124;03m                    \"token_usage\": {\u001b[39;00m\n\u001b[0;32m   1707\u001b[0m \u001b[38;5;124;03m                        \"completion_tokens\": 5,\u001b[39;00m\n\u001b[0;32m   1708\u001b[0m \u001b[38;5;124;03m                        \"prompt_tokens\": 31,\u001b[39;00m\n\u001b[0;32m   1709\u001b[0m \u001b[38;5;124;03m                        \"total_tokens\": 36,\u001b[39;00m\n\u001b[0;32m   1710\u001b[0m \u001b[38;5;124;03m                    },\u001b[39;00m\n\u001b[0;32m   1711\u001b[0m \u001b[38;5;124;03m                    \"model_name\": \"gpt-4o\",\u001b[39;00m\n\u001b[0;32m   1712\u001b[0m \u001b[38;5;124;03m                    \"system_fingerprint\": \"fp_43dfabdef1\",\u001b[39;00m\n\u001b[0;32m   1713\u001b[0m \u001b[38;5;124;03m                    \"finish_reason\": \"stop\",\u001b[39;00m\n\u001b[0;32m   1714\u001b[0m \u001b[38;5;124;03m                    \"logprobs\": None,\u001b[39;00m\n\u001b[0;32m   1715\u001b[0m \u001b[38;5;124;03m                },\u001b[39;00m\n\u001b[0;32m   1716\u001b[0m \u001b[38;5;124;03m                id=\"run-012cffe2-5d3d-424d-83b5-51c6d4a593d1-0\",\u001b[39;00m\n\u001b[0;32m   1717\u001b[0m \u001b[38;5;124;03m                usage_metadata={\"input_tokens\": 31, \"output_tokens\": 5, \"total_tokens\": 36},\u001b[39;00m\n\u001b[0;32m   1718\u001b[0m \u001b[38;5;124;03m            )\u001b[39;00m\n\u001b[0;32m   1719\u001b[0m \n\u001b[0;32m   1720\u001b[0m \u001b[38;5;124;03m    .. dropdown:: Tool calling\u001b[39;00m\n\u001b[0;32m   1721\u001b[0m \n\u001b[0;32m   1722\u001b[0m \u001b[38;5;124;03m        .. code-block:: python\u001b[39;00m\n\u001b[0;32m   1723\u001b[0m \n\u001b[0;32m   1724\u001b[0m \u001b[38;5;124;03m            from pydantic import BaseModel, Field\u001b[39;00m\n\u001b[0;32m   1725\u001b[0m \n\u001b[0;32m   1726\u001b[0m \n\u001b[0;32m   1727\u001b[0m \u001b[38;5;124;03m            class GetWeather(BaseModel):\u001b[39;00m\n\u001b[0;32m   1728\u001b[0m \u001b[38;5;124;03m                '''Get the current weather in a given location'''\u001b[39;00m\n\u001b[0;32m   1729\u001b[0m \n\u001b[0;32m   1730\u001b[0m \u001b[38;5;124;03m                location: str = Field(\u001b[39;00m\n\u001b[0;32m   1731\u001b[0m \u001b[38;5;124;03m                    ..., description=\"The city and state, e.g. San Francisco, CA\"\u001b[39;00m\n\u001b[0;32m   1732\u001b[0m \u001b[38;5;124;03m                )\u001b[39;00m\n\u001b[0;32m   1733\u001b[0m \n\u001b[0;32m   1734\u001b[0m \n\u001b[0;32m   1735\u001b[0m \u001b[38;5;124;03m            class GetPopulation(BaseModel):\u001b[39;00m\n\u001b[0;32m   1736\u001b[0m \u001b[38;5;124;03m                '''Get the current population in a given location'''\u001b[39;00m\n\u001b[0;32m   1737\u001b[0m \n\u001b[0;32m   1738\u001b[0m \u001b[38;5;124;03m                location: str = Field(\u001b[39;00m\n\u001b[0;32m   1739\u001b[0m \u001b[38;5;124;03m                    ..., description=\"The city and state, e.g. San Francisco, CA\"\u001b[39;00m\n\u001b[0;32m   1740\u001b[0m \u001b[38;5;124;03m                )\u001b[39;00m\n\u001b[0;32m   1741\u001b[0m \n\u001b[0;32m   1742\u001b[0m \n\u001b[0;32m   1743\u001b[0m \u001b[38;5;124;03m            llm_with_tools = llm.bind_tools(\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;124;03m                [GetWeather, GetPopulation]\u001b[39;00m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;124;03m                # strict = True  # enforce tool args schema is respected\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;124;03m            )\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;124;03m            ai_msg = llm_with_tools.invoke(\u001b[39;00m\n\u001b[0;32m   1748\u001b[0m \u001b[38;5;124;03m                \"Which city is hotter today and which is bigger: LA or NY?\"\u001b[39;00m\n\u001b[0;32m   1749\u001b[0m \u001b[38;5;124;03m            )\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;124;03m            ai_msg.tool_calls\u001b[39;00m\n\u001b[0;32m   1751\u001b[0m \n\u001b[0;32m   1752\u001b[0m \u001b[38;5;124;03m        .. code-block:: python\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m \n\u001b[0;32m   1754\u001b[0m \u001b[38;5;124;03m            [\u001b[39;00m\n\u001b[0;32m   1755\u001b[0m \u001b[38;5;124;03m                {\u001b[39;00m\n\u001b[0;32m   1756\u001b[0m \u001b[38;5;124;03m                    \"name\": \"GetWeather\",\u001b[39;00m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;124;03m                    \"args\": {\"location\": \"Los Angeles, CA\"},\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;124;03m                    \"id\": \"call_6XswGD5Pqk8Tt5atYr7tfenU\",\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;124;03m                },\u001b[39;00m\n\u001b[0;32m   1760\u001b[0m \u001b[38;5;124;03m                {\u001b[39;00m\n\u001b[1;32m-> 1761\u001b[0m \u001b[38;5;124;03m                    \"name\": \"GetWeather\",\u001b[39;00m\n\u001b[0;32m   1762\u001b[0m \u001b[38;5;124;03m                    \"args\": {\"location\": \"New York, NY\"},\u001b[39;00m\n\u001b[0;32m   1763\u001b[0m \u001b[38;5;124;03m                    \"id\": \"call_ZVL15vA8Y7kXqOy3dtmQgeCi\",\u001b[39;00m\n\u001b[0;32m   1764\u001b[0m \u001b[38;5;124;03m                },\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;124;03m                {\u001b[39;00m\n\u001b[0;32m   1766\u001b[0m \u001b[38;5;124;03m                    \"name\": \"GetPopulation\",\u001b[39;00m\n\u001b[0;32m   1767\u001b[0m \u001b[38;5;124;03m                    \"args\": {\"location\": \"Los Angeles, CA\"},\u001b[39;00m\n\u001b[0;32m   1768\u001b[0m \u001b[38;5;124;03m                    \"id\": \"call_49CFW8zqC9W7mh7hbMLSIrXw\",\u001b[39;00m\n\u001b[0;32m   1769\u001b[0m \u001b[38;5;124;03m                },\u001b[39;00m\n\u001b[0;32m   1770\u001b[0m \u001b[38;5;124;03m                {\u001b[39;00m\n\u001b[0;32m   1771\u001b[0m \u001b[38;5;124;03m                    \"name\": \"GetPopulation\",\u001b[39;00m\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;124;03m                    \"args\": {\"location\": \"New York, NY\"},\u001b[39;00m\n\u001b[0;32m   1773\u001b[0m \u001b[38;5;124;03m                    \"id\": \"call_6ghfKxV264jEfe1mRIkS3PE7\",\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;124;03m                },\u001b[39;00m\n\u001b[0;32m   1775\u001b[0m \u001b[38;5;124;03m            ]\u001b[39;00m\n\u001b[0;32m   1776\u001b[0m \n\u001b[0;32m   1777\u001b[0m \u001b[38;5;124;03m        Note that ``openai >= 1.32`` supports a ``parallel_tool_calls`` parameter\u001b[39;00m\n\u001b[0;32m   1778\u001b[0m \u001b[38;5;124;03m        that defaults to ``True``. This parameter can be set to ``False`` to\u001b[39;00m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;124;03m        disable parallel tool calls:\u001b[39;00m\n\u001b[0;32m   1780\u001b[0m \n\u001b[0;32m   1781\u001b[0m \u001b[38;5;124;03m        .. code-block:: python\u001b[39;00m\n\u001b[0;32m   1782\u001b[0m \n\u001b[0;32m   1783\u001b[0m \u001b[38;5;124;03m            ai_msg = llm_with_tools.invoke(\u001b[39;00m\n\u001b[0;32m   1784\u001b[0m \u001b[38;5;124;03m                \"What is the weather in LA and NY?\", parallel_tool_calls=False\u001b[39;00m\n\u001b[0;32m   1785\u001b[0m \u001b[38;5;124;03m            )\u001b[39;00m\n\u001b[0;32m   1786\u001b[0m \u001b[38;5;124;03m            ai_msg.tool_calls\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m \n\u001b[0;32m   1788\u001b[0m \u001b[38;5;124;03m        .. code-block:: python\u001b[39;00m\n\u001b[0;32m   1789\u001b[0m \n\u001b[0;32m   1790\u001b[0m \u001b[38;5;124;03m            [\u001b[39;00m\n\u001b[0;32m   1791\u001b[0m \u001b[38;5;124;03m                {\u001b[39;00m\n\u001b[0;32m   1792\u001b[0m \u001b[38;5;124;03m                    \"name\": \"GetWeather\",\u001b[39;00m\n\u001b[0;32m   1793\u001b[0m \u001b[38;5;124;03m                    \"args\": {\"location\": \"Los Angeles, CA\"},\u001b[39;00m\n\u001b[0;32m   1794\u001b[0m \u001b[38;5;124;03m                    \"id\": \"call_4OoY0ZR99iEvC7fevsH8Uhtz\",\u001b[39;00m\n\u001b[0;32m   1795\u001b[0m \u001b[38;5;124;03m                }\u001b[39;00m\n\u001b[0;32m   1796\u001b[0m \u001b[38;5;124;03m            ]\u001b[39;00m\n\u001b[0;32m   1797\u001b[0m \n\u001b[0;32m   1798\u001b[0m \u001b[38;5;124;03m        Like other runtime parameters, ``parallel_tool_calls`` can be bound to a model\u001b[39;00m\n\u001b[0;32m   1799\u001b[0m \u001b[38;5;124;03m        using ``llm.bind(parallel_tool_calls=False)`` or during instantiation by\u001b[39;00m\n\u001b[0;32m   1800\u001b[0m \u001b[38;5;124;03m        setting ``model_kwargs``.\u001b[39;00m\n\u001b[0;32m   1801\u001b[0m \n\u001b[0;32m   1802\u001b[0m \u001b[38;5;124;03m        See ``ChatOpenAI.bind_tools()`` method for more.\u001b[39;00m\n\u001b[0;32m   1803\u001b[0m \n\u001b[0;32m   1804\u001b[0m \u001b[38;5;124;03m    .. dropdown:: Structured output\u001b[39;00m\n\u001b[0;32m   1805\u001b[0m \n\u001b[0;32m   1806\u001b[0m \u001b[38;5;124;03m        .. code-block:: python\u001b[39;00m\n\u001b[0;32m   1807\u001b[0m \n\u001b[0;32m   1808\u001b[0m \u001b[38;5;124;03m            from typing import Optional\u001b[39;00m\n\u001b[0;32m   1809\u001b[0m \n\u001b[0;32m   1810\u001b[0m \u001b[38;5;124;03m            from pydantic import BaseModel, Field\u001b[39;00m\n\u001b[0;32m   1811\u001b[0m \n\u001b[0;32m   1812\u001b[0m \n\u001b[0;32m   1813\u001b[0m \u001b[38;5;124;03m            class Joke(BaseModel):\u001b[39;00m\n\u001b[0;32m   1814\u001b[0m \u001b[38;5;124;03m                '''Joke to tell user.'''\u001b[39;00m\n\u001b[0;32m   1815\u001b[0m \n\u001b[0;32m   1816\u001b[0m \u001b[38;5;124;03m                setup: str = Field(description=\"The setup of the joke\")\u001b[39;00m\n\u001b[0;32m   1817\u001b[0m \u001b[38;5;124;03m                punchline: str = Field(description=\"The punchline to the joke\")\u001b[39;00m\n\u001b[0;32m   1818\u001b[0m \u001b[38;5;124;03m                rating: Optional[int] = Field(description=\"How funny the joke is, from 1 to 10\")\u001b[39;00m\n\u001b[0;32m   1819\u001b[0m \n\u001b[0;32m   1820\u001b[0m \n\u001b[0;32m   1821\u001b[0m \u001b[38;5;124;03m            structured_llm = llm.with_structured_output(Joke)\u001b[39;00m\n\u001b[0;32m   1822\u001b[0m \u001b[38;5;124;03m            structured_llm.invoke(\"Tell me a joke about cats\")\u001b[39;00m\n\u001b[0;32m   1823\u001b[0m \n\u001b[0;32m   1824\u001b[0m \u001b[38;5;124;03m        .. code-block:: python\u001b[39;00m\n\u001b[0;32m   1825\u001b[0m \n\u001b[0;32m   1826\u001b[0m \u001b[38;5;124;03m            Joke(\u001b[39;00m\n\u001b[0;32m   1827\u001b[0m \u001b[38;5;124;03m                setup=\"Why was the cat sitting on the computer?\",\u001b[39;00m\n\u001b[0;32m   1828\u001b[0m \u001b[38;5;124;03m                punchline=\"To keep an eye on the mouse!\",\u001b[39;00m\n\u001b[0;32m   1829\u001b[0m \u001b[38;5;124;03m                rating=None,\u001b[39;00m\n\u001b[0;32m   1830\u001b[0m \u001b[38;5;124;03m            )\u001b[39;00m\n\u001b[0;32m   1831\u001b[0m \n\u001b[0;32m   1832\u001b[0m \u001b[38;5;124;03m        See ``ChatOpenAI.with_structured_output()`` for more.\u001b[39;00m\n\u001b[0;32m   1833\u001b[0m \n\u001b[0;32m   1834\u001b[0m \u001b[38;5;124;03m    .. dropdown:: JSON mode\u001b[39;00m\n\u001b[0;32m   1835\u001b[0m \n\u001b[0;32m   1836\u001b[0m \u001b[38;5;124;03m        .. code-block:: python\u001b[39;00m\n\u001b[0;32m   1837\u001b[0m \n\u001b[0;32m   1838\u001b[0m \u001b[38;5;124;03m            json_llm = llm.bind(response_format={\"type\": \"json_object\"})\u001b[39;00m\n\u001b[0;32m   1839\u001b[0m \u001b[38;5;124;03m            ai_msg = json_llm.invoke(\u001b[39;00m\n\u001b[0;32m   1840\u001b[0m \u001b[38;5;124;03m                \"Return a JSON object with key 'random_ints' and a value of 10 random ints in [0-99]\"\u001b[39;00m\n\u001b[0;32m   1841\u001b[0m \u001b[38;5;124;03m            )\u001b[39;00m\n\u001b[0;32m   1842\u001b[0m \u001b[38;5;124;03m            ai_msg.content\u001b[39;00m\n\u001b[0;32m   1843\u001b[0m \n\u001b[0;32m   1844\u001b[0m \u001b[38;5;124;03m        .. code-block:: python\u001b[39;00m\n\u001b[0;32m   1845\u001b[0m \n\u001b[0;32m   1846\u001b[0m \u001b[38;5;124;03m            '\\\\n{\\\\n  \"random_ints\": [23, 87, 45, 12, 78, 34, 56, 90, 11, 67]\\\\n}'\u001b[39;00m\n\u001b[0;32m   1847\u001b[0m \n\u001b[0;32m   1848\u001b[0m \u001b[38;5;124;03m    .. dropdown:: Image input\u001b[39;00m\n\u001b[0;32m   1849\u001b[0m \n\u001b[0;32m   1850\u001b[0m \u001b[38;5;124;03m        .. code-block:: python\u001b[39;00m\n\u001b[0;32m   1851\u001b[0m \n\u001b[0;32m   1852\u001b[0m \u001b[38;5;124;03m            import base64\u001b[39;00m\n\u001b[0;32m   1853\u001b[0m \u001b[38;5;124;03m            import httpx\u001b[39;00m\n\u001b[0;32m   1854\u001b[0m \u001b[38;5;124;03m            from langchain_core.messages import HumanMessage\u001b[39;00m\n\u001b[0;32m   1855\u001b[0m \n\u001b[0;32m   1856\u001b[0m \u001b[38;5;124;03m            image_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\"\u001b[39;00m\n\u001b[0;32m   1857\u001b[0m \u001b[38;5;124;03m            image_data = base64.b64encode(httpx.get(image_url).content).decode(\"utf-8\")\u001b[39;00m\n\u001b[0;32m   1858\u001b[0m \u001b[38;5;124;03m            message = HumanMessage(\u001b[39;00m\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;124;03m                content=[\u001b[39;00m\n\u001b[0;32m   1860\u001b[0m \u001b[38;5;124;03m                    {\"type\": \"text\", \"text\": \"describe the weather in this image\"},\u001b[39;00m\n\u001b[0;32m   1861\u001b[0m \u001b[38;5;124;03m                    {\u001b[39;00m\n\u001b[0;32m   1862\u001b[0m \u001b[38;5;124;03m                        \"type\": \"image_url\",\u001b[39;00m\n\u001b[0;32m   1863\u001b[0m \u001b[38;5;124;03m                        \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_data}\"},\u001b[39;00m\n\u001b[0;32m   1864\u001b[0m \u001b[38;5;124;03m                    },\u001b[39;00m\n\u001b[0;32m   1865\u001b[0m \u001b[38;5;124;03m                ]\u001b[39;00m\n\u001b[0;32m   1866\u001b[0m \u001b[38;5;124;03m            )\u001b[39;00m\n\u001b[0;32m   1867\u001b[0m \u001b[38;5;124;03m            ai_msg = llm.invoke([message])\u001b[39;00m\n\u001b[0;32m   1868\u001b[0m \u001b[38;5;124;03m            ai_msg.content\u001b[39;00m\n\u001b[0;32m   1869\u001b[0m \n\u001b[0;32m   1870\u001b[0m \u001b[38;5;124;03m        .. code-block:: python\u001b[39;00m\n\u001b[0;32m   1871\u001b[0m \n\u001b[0;32m   1872\u001b[0m \u001b[38;5;124;03m            \"The weather in the image appears to be clear and pleasant. The sky is mostly blue with scattered, light clouds, suggesting a sunny day with minimal cloud cover. There is no indication of rain or strong winds, and the overall scene looks bright and calm. The lush green grass and clear visibility further indicate good weather conditions.\"\u001b[39;00m\n\u001b[0;32m   1873\u001b[0m \n\u001b[0;32m   1874\u001b[0m \u001b[38;5;124;03m    .. dropdown:: Token usage\u001b[39;00m\n\u001b[0;32m   1875\u001b[0m \n\u001b[0;32m   1876\u001b[0m \u001b[38;5;124;03m        .. code-block:: python\u001b[39;00m\n\u001b[0;32m   1877\u001b[0m \n\u001b[0;32m   1878\u001b[0m \u001b[38;5;124;03m            ai_msg = llm.invoke(messages)\u001b[39;00m\n\u001b[0;32m   1879\u001b[0m \u001b[38;5;124;03m            ai_msg.usage_metadata\u001b[39;00m\n\u001b[0;32m   1880\u001b[0m \n\u001b[0;32m   1881\u001b[0m \u001b[38;5;124;03m        .. code-block:: python\u001b[39;00m\n\u001b[0;32m   1882\u001b[0m \n\u001b[0;32m   1883\u001b[0m \u001b[38;5;124;03m            {\"input_tokens\": 28, \"output_tokens\": 5, \"total_tokens\": 33}\u001b[39;00m\n\u001b[0;32m   1884\u001b[0m \n\u001b[0;32m   1885\u001b[0m \u001b[38;5;124;03m        When streaming, set the ``stream_usage`` kwarg:\u001b[39;00m\n\u001b[0;32m   1886\u001b[0m \n\u001b[0;32m   1887\u001b[0m \u001b[38;5;124;03m        .. code-block:: python\u001b[39;00m\n\u001b[0;32m   1888\u001b[0m \n\u001b[0;32m   1889\u001b[0m \u001b[38;5;124;03m            stream = llm.stream(messages, stream_usage=True)\u001b[39;00m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;124;03m            full = next(stream)\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m \u001b[38;5;124;03m            for chunk in stream:\u001b[39;00m\n\u001b[0;32m   1892\u001b[0m \u001b[38;5;124;03m                full += chunk\u001b[39;00m\n\u001b[0;32m   1893\u001b[0m \u001b[38;5;124;03m            full.usage_metadata\u001b[39;00m\n\u001b[0;32m   1894\u001b[0m \n\u001b[0;32m   1895\u001b[0m \u001b[38;5;124;03m        .. code-block:: python\u001b[39;00m\n\u001b[0;32m   1896\u001b[0m \n\u001b[0;32m   1897\u001b[0m \u001b[38;5;124;03m            {\"input_tokens\": 28, \"output_tokens\": 5, \"total_tokens\": 33}\u001b[39;00m\n\u001b[0;32m   1898\u001b[0m \n\u001b[0;32m   1899\u001b[0m \u001b[38;5;124;03m        Alternatively, setting ``stream_usage`` when instantiating the model can be\u001b[39;00m\n\u001b[0;32m   1900\u001b[0m \u001b[38;5;124;03m        useful when incorporating ``ChatOpenAI`` into LCEL chains-- or when using\u001b[39;00m\n\u001b[0;32m   1901\u001b[0m \u001b[38;5;124;03m        methods like ``.with_structured_output``, which generate chains under the\u001b[39;00m\n\u001b[0;32m   1902\u001b[0m \u001b[38;5;124;03m        hood.\u001b[39;00m\n\u001b[0;32m   1903\u001b[0m \n\u001b[0;32m   1904\u001b[0m \u001b[38;5;124;03m        .. code-block:: python\u001b[39;00m\n\u001b[0;32m   1905\u001b[0m \n\u001b[0;32m   1906\u001b[0m \u001b[38;5;124;03m            llm = ChatOpenAI(model=\"gpt-4o\", stream_usage=True)\u001b[39;00m\n\u001b[0;32m   1907\u001b[0m \u001b[38;5;124;03m            structured_llm = llm.with_structured_output(...)\u001b[39;00m\n\u001b[0;32m   1908\u001b[0m \n\u001b[0;32m   1909\u001b[0m \u001b[38;5;124;03m    .. dropdown:: Logprobs\u001b[39;00m\n\u001b[0;32m   1910\u001b[0m \n\u001b[0;32m   1911\u001b[0m \u001b[38;5;124;03m        .. code-block:: python\u001b[39;00m\n\u001b[0;32m   1912\u001b[0m \n\u001b[0;32m   1913\u001b[0m \u001b[38;5;124;03m            logprobs_llm = llm.bind(logprobs=True)\u001b[39;00m\n\u001b[0;32m   1914\u001b[0m \u001b[38;5;124;03m            ai_msg = logprobs_llm.invoke(messages)\u001b[39;00m\n\u001b[0;32m   1915\u001b[0m \u001b[38;5;124;03m            ai_msg.response_metadata[\"logprobs\"]\u001b[39;00m\n\u001b[0;32m   1916\u001b[0m \n\u001b[0;32m   1917\u001b[0m \u001b[38;5;124;03m        .. code-block:: python\u001b[39;00m\n\u001b[0;32m   1918\u001b[0m \n\u001b[0;32m   1919\u001b[0m \u001b[38;5;124;03m            {\u001b[39;00m\n\u001b[0;32m   1920\u001b[0m \u001b[38;5;124;03m                \"content\": [\u001b[39;00m\n\u001b[0;32m   1921\u001b[0m \u001b[38;5;124;03m                    {\u001b[39;00m\n\u001b[0;32m   1922\u001b[0m \u001b[38;5;124;03m                        \"token\": \"J\",\u001b[39;00m\n\u001b[0;32m   1923\u001b[0m \u001b[38;5;124;03m                        \"bytes\": [74],\u001b[39;00m\n\u001b[0;32m   1924\u001b[0m \u001b[38;5;124;03m                        \"logprob\": -4.9617593e-06,\u001b[39;00m\n\u001b[0;32m   1925\u001b[0m \u001b[38;5;124;03m                        \"top_logprobs\": [],\u001b[39;00m\n\u001b[0;32m   1926\u001b[0m \u001b[38;5;124;03m                    },\u001b[39;00m\n\u001b[0;32m   1927\u001b[0m \u001b[38;5;124;03m                    {\u001b[39;00m\n\u001b[0;32m   1928\u001b[0m \u001b[38;5;124;03m                        \"token\": \"'adore\",\u001b[39;00m\n\u001b[0;32m   1929\u001b[0m \u001b[38;5;124;03m                        \"bytes\": [39, 97, 100, 111, 114, 101],\u001b[39;00m\n\u001b[0;32m   1930\u001b[0m \u001b[38;5;124;03m                        \"logprob\": -0.25202933,\u001b[39;00m\n\u001b[0;32m   1931\u001b[0m \u001b[38;5;124;03m                        \"top_logprobs\": [],\u001b[39;00m\n\u001b[0;32m   1932\u001b[0m \u001b[38;5;124;03m                    },\u001b[39;00m\n\u001b[0;32m   1933\u001b[0m \u001b[38;5;124;03m                    {\u001b[39;00m\n\u001b[0;32m   1934\u001b[0m \u001b[38;5;124;03m                        \"token\": \" la\",\u001b[39;00m\n\u001b[0;32m   1935\u001b[0m \u001b[38;5;124;03m                        \"bytes\": [32, 108, 97],\u001b[39;00m\n\u001b[0;32m   1936\u001b[0m \u001b[38;5;124;03m                        \"logprob\": -0.20141791,\u001b[39;00m\n\u001b[0;32m   1937\u001b[0m \u001b[38;5;124;03m                        \"top_logprobs\": [],\u001b[39;00m\n\u001b[0;32m   1938\u001b[0m \u001b[38;5;124;03m                    },\u001b[39;00m\n\u001b[0;32m   1939\u001b[0m \u001b[38;5;124;03m                    {\u001b[39;00m\n\u001b[0;32m   1940\u001b[0m \u001b[38;5;124;03m                        \"token\": \" programmation\",\u001b[39;00m\n\u001b[0;32m   1941\u001b[0m \u001b[38;5;124;03m                        \"bytes\": [\u001b[39;00m\n\u001b[0;32m   1942\u001b[0m \u001b[38;5;124;03m                            32,\u001b[39;00m\n\u001b[0;32m   1943\u001b[0m \u001b[38;5;124;03m                            112,\u001b[39;00m\n\u001b[0;32m   1944\u001b[0m \u001b[38;5;124;03m                            114,\u001b[39;00m\n\u001b[0;32m   1945\u001b[0m \u001b[38;5;124;03m                            111,\u001b[39;00m\n\u001b[0;32m   1946\u001b[0m \u001b[38;5;124;03m                            103,\u001b[39;00m\n\u001b[0;32m   1947\u001b[0m \u001b[38;5;124;03m                            114,\u001b[39;00m\n\u001b[0;32m   1948\u001b[0m \u001b[38;5;124;03m                            97,\u001b[39;00m\n\u001b[0;32m   1949\u001b[0m \u001b[38;5;124;03m                            109,\u001b[39;00m\n\u001b[0;32m   1950\u001b[0m \u001b[38;5;124;03m                            109,\u001b[39;00m\n\u001b[0;32m   1951\u001b[0m \u001b[38;5;124;03m                            97,\u001b[39;00m\n\u001b[0;32m   1952\u001b[0m \u001b[38;5;124;03m                            116,\u001b[39;00m\n\u001b[0;32m   1953\u001b[0m \u001b[38;5;124;03m                            105,\u001b[39;00m\n\u001b[0;32m   1954\u001b[0m \u001b[38;5;124;03m                            111,\u001b[39;00m\n\u001b[0;32m   1955\u001b[0m \u001b[38;5;124;03m                            110,\u001b[39;00m\n\u001b[0;32m   1956\u001b[0m \u001b[38;5;124;03m                        ],\u001b[39;00m\n\u001b[0;32m   1957\u001b[0m \u001b[38;5;124;03m                        \"logprob\": -1.9361265e-07,\u001b[39;00m\n\u001b[0;32m   1958\u001b[0m \u001b[38;5;124;03m                        \"top_logprobs\": [],\u001b[39;00m\n\u001b[0;32m   1959\u001b[0m \u001b[38;5;124;03m                    },\u001b[39;00m\n\u001b[0;32m   1960\u001b[0m \u001b[38;5;124;03m                    {\u001b[39;00m\n\u001b[0;32m   1961\u001b[0m \u001b[38;5;124;03m                        \"token\": \".\",\u001b[39;00m\n\u001b[0;32m   1962\u001b[0m \u001b[38;5;124;03m                        \"bytes\": [46],\u001b[39;00m\n\u001b[0;32m   1963\u001b[0m \u001b[38;5;124;03m                        \"logprob\": -1.2233183e-05,\u001b[39;00m\n\u001b[0;32m   1964\u001b[0m \u001b[38;5;124;03m                        \"top_logprobs\": [],\u001b[39;00m\n\u001b[0;32m   1965\u001b[0m \u001b[38;5;124;03m                    },\u001b[39;00m\n\u001b[0;32m   1966\u001b[0m \u001b[38;5;124;03m                ]\u001b[39;00m\n\u001b[0;32m   1967\u001b[0m \u001b[38;5;124;03m            }\u001b[39;00m\n\u001b[0;32m   1968\u001b[0m \n\u001b[0;32m   1969\u001b[0m \u001b[38;5;124;03m    .. dropdown:: Response metadata\u001b[39;00m\n\u001b[0;32m   1970\u001b[0m \n\u001b[0;32m   1971\u001b[0m \u001b[38;5;124;03m        .. code-block:: python\u001b[39;00m\n\u001b[0;32m   1972\u001b[0m \n\u001b[0;32m   1973\u001b[0m \u001b[38;5;124;03m            ai_msg = llm.invoke(messages)\u001b[39;00m\n\u001b[0;32m   1974\u001b[0m \u001b[38;5;124;03m            ai_msg.response_metadata\u001b[39;00m\n\u001b[0;32m   1975\u001b[0m \n\u001b[0;32m   1976\u001b[0m \u001b[38;5;124;03m        .. code-block:: python\u001b[39;00m\n\u001b[0;32m   1977\u001b[0m \n\u001b[0;32m   1978\u001b[0m \u001b[38;5;124;03m            {\u001b[39;00m\n\u001b[0;32m   1979\u001b[0m \u001b[38;5;124;03m                \"token_usage\": {\u001b[39;00m\n\u001b[0;32m   1980\u001b[0m \u001b[38;5;124;03m                    \"completion_tokens\": 5,\u001b[39;00m\n\u001b[0;32m   1981\u001b[0m \u001b[38;5;124;03m                    \"prompt_tokens\": 28,\u001b[39;00m\n\u001b[0;32m   1982\u001b[0m \u001b[38;5;124;03m                    \"total_tokens\": 33,\u001b[39;00m\n\u001b[0;32m   1983\u001b[0m \u001b[38;5;124;03m                },\u001b[39;00m\n\u001b[0;32m   1984\u001b[0m \u001b[38;5;124;03m                \"model_name\": \"gpt-4o\",\u001b[39;00m\n\u001b[0;32m   1985\u001b[0m \u001b[38;5;124;03m                \"system_fingerprint\": \"fp_319be4768e\",\u001b[39;00m\n\u001b[0;32m   1986\u001b[0m \u001b[38;5;124;03m                \"finish_reason\": \"stop\",\u001b[39;00m\n\u001b[0;32m   1987\u001b[0m \u001b[38;5;124;03m                \"logprobs\": None,\u001b[39;00m\n\u001b[0;32m   1988\u001b[0m \u001b[38;5;124;03m            }\u001b[39;00m\n\u001b[0;32m   1989\u001b[0m \n\u001b[0;32m   1990\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[0;32m   1992\u001b[0m     stream_usage: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1993\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Whether to include usage metadata in streaming output. If True, additional\u001b[39;00m\n\u001b[0;32m   1994\u001b[0m \u001b[38;5;124;03m    message chunks will be generated during the stream including usage metadata.\u001b[39;00m\n\u001b[0;32m   1995\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: get_num_tokens_from_messages() is not presently implemented for model llama-3.1-8b-instant. See https://platform.openai.com/docs/guides/text-generation/managing-tokens for information on how messages are converted to tokens."
     ]
    }
   ],
   "source": [
    "# Paso 1: Introducción\n",
    "conversation.predict(input=\"Hola, soy Luciano y vivo en Argentina. Trabajo como Analista Programador.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
